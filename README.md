# TechNowNet: A Contemporary Multi-Domain Knowledge Semantic Network

TechNowNet is a multi-domain semantic network designed to integrate science, technology, and market-and-society knowledge.
It employs cross-domain embedding fusion to align knowledge from heterogeneous sources (Scopus, PATSTAT, NewsCatcher) into a single semantic space.

---

## ðŸ“‚ Project Structure

```text
TechNowNet/
â”œâ”€â”€ data/                            # Original source data (Scopus, PATSTAT, NewsCatcher)
â”œâ”€â”€ dataset/                         # Preprocessed results (.lem) and training corpora (.docu)
â”œâ”€â”€ model/                           # Trained FastText models and integrated vectors
â”œâ”€â”€ scripts/                         # Module source codes
â”‚   â”œâ”€â”€ prep-technology.py           # Patent data preprocessing
â”‚   â”œâ”€â”€ prep-science.py              # Scopus data preprocessing
â”‚   â”œâ”€â”€ prep-market-and-society.py   # Market-and-society data preprocessing
â”‚   â”œâ”€â”€ utils.py                     # Utility for preprocessing
â”‚   â”œâ”€â”€ docu.py                      # Training corpus (.docu) generation
â”‚   â”œâ”€â”€ train.py                     # FastText domain-specific training
â”‚   â””â”€â”€ Aligner.py                   # OPA-based cross-domain fusion (TechNowNet)
â”œâ”€â”€ main.py                          # Integrated execution script
â”œâ”€â”€ requirements.txt                 # Required library list
â””â”€â”€ README.md                        # Project documentation
```

---

## ðŸš€ Execution Pipeline

TechNowNet processes knowledge through a circular structure of generation, application, and cognition.

### 1. Preprocessing (`prep-*.py`)
Extracts and cleans terms from specific domains for the period 2019-2023
* Performs tokenization, N-gram generation, and lemmatization.
* Extracts shared terms (appearing in all domains) and domain-specific terms.

### 2. Training Corpus Generation (`docu.py`)
Converts `.lem` files into `.docu` format suitable for large-scale embedding training.
* Filters out non-English characters to ensure linguistic consistency.

### 3. Domain-specific Embedding (`train.py`)
Trains independent **FastText** models for each domain to preserve morphological and subword information.
* Independent training maximizes the retention of domain-specific uniqueness.

### 4. Cross-domain Fusion (`Aligner.py`)
Uses an **extended Orthogonal Procrustes Analysis (OPA)** to align multiple embedding spaces.
* **Shared Knowledge Fusion**: Aligns shared term matrices towards a mean matrix.
* **Domain-specific Alignment**: Positions unique terms within the fused space while preserving their specialized features.

---

## ðŸ“ˆ Key Achievements
* **Scalability**: Achieved approximately 4.4 times greater term coverage (17.7 million terms) than existing methods.
* **Quality**: Demonstrated a 7-percentage-point higher performance (**SOTA**) in Technology-Term Relevance (TTR) tasks.
* **Contemporaneity**: Captures the latest trends in AI, quantum computing, and biotechnology.

---

## ðŸ›  Requirements
* Python 3.8+
* Gensim 4.0+
* NLTK
* Scipy & Scikit-learn

## ðŸ“œ Citation
```text
Lee, J., Lim, J., & Yang, H. (2026). TechNowNet: A Contemporary Multi-Domain Knowledge Semantic Network through Cross-Domain Embedding Fusion.
```
